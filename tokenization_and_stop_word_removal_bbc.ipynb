{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you get the data from here http://mlg.ucd.ie/datasets/bbc.html and put it into the data folder when unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bbc = \"./data/bbc\"\n",
    "path_to_bbcsport = \"./data/bbcsport\"\n",
    "\n",
    "bbc_business = os.path.join(path_to_bbc, \"business\")\n",
    "bbc_entertainment = os.path.join(path_to_bbc, \"entertainment\")\n",
    "bbc_tech = os.path.join(path_to_bbc, \"tech\")\n",
    "bbc_politics = os.path.join(path_to_bbc, \"politics\")\n",
    "bbc_sport = os.path.join(path_to_bbc, \"sport\")\n",
    "\n",
    "bbcsport_athletics = os.path.join(path_to_bbcsport, \"athletics\")\n",
    "bbcsport_cricket = os.path.join(path_to_bbcsport, \"cricket\")\n",
    "bbcsport_football = os.path.join(path_to_bbcsport, \"football\")\n",
    "bbcsport_rugby = os.path.join(path_to_bbcsport, \"rugby\")\n",
    "bbcsport_tennis = os.path.join(path_to_bbcsport, \"tennis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for path in [bbc_business, bbc_entertainment, bbc_tech, bbc_politics, bbc_sport, bbcsport_athletics, bbcsport_cricket, bbcsport_football,bbcsport_rugby, bbcsport_tennis]:\n",
    "    for f_name in os.listdir(path):\n",
    "        with open(os.path.join(path, f_name), encoding='utf-8', errors='ignore') as f: \n",
    "            data = f.readlines() \n",
    "        single_file_data = [x for x in data if len(x)>5]\n",
    "        sentences+=single_file_data\n",
    "        labels.append(path.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of sentences : 16953\n",
      "\n",
      "\n",
      "Possible labels : 10\n",
      "\n",
      "Label  1 ---> athletics\n",
      "Label  2 ---> business\n",
      "Label  3 ---> cricket\n",
      "Label  4 ---> entertainment\n",
      "Label  5 ---> football\n",
      "Label  6 ---> politics\n",
      "Label  7 ---> rugby\n",
      "Label  8 ---> sport\n",
      "Label  9 ---> tech\n",
      "Label 10 ---> tennis\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of sentences : {}\\n\\n\".format(len(sentences)))\n",
    "print(\"Possible labels : {}\\n\".format(len(sorted(set(labels)))))\n",
    "\n",
    "for i, label in enumerate(sorted(set(labels))):\n",
    "    print(\"Label {:2d} ---> {}\".format(i+1, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(list_of_sentences, stop_word_array):\n",
    "    for stop_word in stop_word_array:\n",
    "        list_of_sentences = [words.replace(\" \"+stop_word+\" \", \" \") for words in list_of_sentences]\n",
    "        list_of_sentences = [words.replace(\"  \", \" \") for words in list_of_sentences]\n",
    "    \n",
    "    return list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a in corpus : False\n",
      "about in corpus : False\n",
      "above in corpus : False\n",
      "after in corpus : False\n",
      "again in corpus : False\n",
      "against in corpus : False\n",
      "all in corpus : False\n",
      "am in corpus : False\n",
      "an in corpus : False\n",
      "and in corpus : False\n",
      "any in corpus : False\n",
      "are in corpus : False\n",
      "as in corpus : False\n",
      "at in corpus : False\n",
      "be in corpus : False\n",
      "because in corpus : False\n",
      "been in corpus : False\n",
      "before in corpus : False\n",
      "being in corpus : False\n",
      "below in corpus : False\n",
      "between in corpus : False\n",
      "both in corpus : False\n",
      "but in corpus : False\n",
      "by in corpus : False\n",
      "could in corpus : False\n",
      "did in corpus : False\n",
      "do in corpus : False\n",
      "does in corpus : False\n",
      "doing in corpus : False\n",
      "down in corpus : False\n",
      "during in corpus : False\n",
      "each in corpus : False\n",
      "few in corpus : False\n",
      "for in corpus : False\n",
      "from in corpus : False\n",
      "further in corpus : False\n",
      "had in corpus : False\n",
      "has in corpus : False\n",
      "have in corpus : False\n",
      "having in corpus : False\n",
      "he in corpus : False\n",
      "he'd in corpus : False\n",
      "he'll in corpus : False\n",
      "he's in corpus : False\n",
      "her in corpus : False\n",
      "here in corpus : False\n",
      "here's in corpus : False\n",
      "hers in corpus : False\n",
      "herself in corpus : False\n",
      "him in corpus : False\n",
      "himself in corpus : False\n",
      "his in corpus : False\n",
      "how in corpus : False\n",
      "how's in corpus : False\n",
      "i in corpus : False\n",
      "i'd in corpus : False\n",
      "i'll in corpus : False\n",
      "i'm in corpus : False\n",
      "i've in corpus : False\n",
      "if in corpus : False\n",
      "in in corpus : False\n",
      "into in corpus : False\n",
      "is in corpus : False\n",
      "it in corpus : False\n",
      "it's in corpus : False\n",
      "its in corpus : False\n",
      "itself in corpus : False\n",
      "let's in corpus : False\n",
      "me in corpus : False\n",
      "more in corpus : False\n",
      "most in corpus : False\n",
      "my in corpus : False\n",
      "myself in corpus : False\n",
      "nor in corpus : False\n",
      "of in corpus : False\n",
      "on in corpus : False\n",
      "once in corpus : False\n",
      "only in corpus : False\n",
      "or in corpus : False\n",
      "other in corpus : False\n",
      "ought in corpus : False\n",
      "our in corpus : False\n",
      "ours in corpus : False\n",
      "ourselves in corpus : False\n",
      "out in corpus : False\n",
      "over in corpus : False\n",
      "own in corpus : False\n",
      "same in corpus : False\n",
      "she in corpus : False\n",
      "she'd in corpus : False\n",
      "she'll in corpus : False\n",
      "she's in corpus : False\n",
      "should in corpus : False\n",
      "so in corpus : False\n",
      "some in corpus : False\n",
      "such in corpus : False\n",
      "than in corpus : False\n",
      "that in corpus : False\n",
      "that's in corpus : False\n",
      "the in corpus : False\n",
      "their in corpus : False\n",
      "theirs in corpus : False\n",
      "them in corpus : False\n",
      "themselves in corpus : False\n",
      "then in corpus : False\n",
      "there in corpus : False\n",
      "there's in corpus : False\n",
      "these in corpus : False\n",
      "they in corpus : False\n",
      "they'd in corpus : False\n",
      "they'll in corpus : False\n",
      "they're in corpus : False\n",
      "they've in corpus : False\n",
      "this in corpus : False\n",
      "those in corpus : False\n",
      "through in corpus : False\n",
      "to in corpus : False\n",
      "too in corpus : False\n",
      "under in corpus : False\n",
      "until in corpus : False\n",
      "up in corpus : False\n",
      "very in corpus : False\n",
      "was in corpus : False\n",
      "we in corpus : False\n",
      "we'd in corpus : False\n",
      "we'll in corpus : False\n",
      "we're in corpus : False\n",
      "we've in corpus : False\n",
      "were in corpus : False\n",
      "what in corpus : False\n",
      "what's in corpus : False\n",
      "when in corpus : False\n",
      "when's in corpus : False\n",
      "where in corpus : False\n",
      "where's in corpus : False\n",
      "which in corpus : False\n",
      "while in corpus : False\n",
      "who in corpus : False\n",
      "who's in corpus : False\n",
      "whom in corpus : False\n",
      "why in corpus : False\n",
      "why's in corpus : False\n",
      "with in corpus : False\n",
      "would in corpus : False\n",
      "you in corpus : False\n",
      "you'd in corpus : False\n",
      "you'll in corpus : False\n",
      "you're in corpus : False\n",
      "you've in corpus : False\n",
      "your in corpus : False\n",
      "yours in corpus : False\n",
      "yourself in corpus : False\n",
      "yourselves in corpus : False\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for stop_word in stop_words:\n",
    "    print(\"{} in corpus : {}\".format(stop_word, stop_word in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words in the word index :\n",
      "\n",
      "33915\n",
      "\n",
      "\n",
      "Words in order of commonality :\n",
      "\n",
      "['<OOV>', 'the', 'to', 'of', 'and', 'a', 'in', 'for', 'is', 'on', 'that', 'it', 'said', 'he', 'was', 'with', 'be', 'but', 'have', 'has', 'at', 'as', 'will', 'by', 'i', 'are', 'his', 'from', 'not', 'we', 'they', 'an', 'this', 'had', 'been', 'their', 'would', 'mr', 'which', 'up', 'who', 'were', 'more', 'year', 'after', 'also', 'one', 'out', 'new', 'its', 'there', 'us', 'all', 'about', 'first', 'over', 'people', 'if', 'when', 'can', 'last', 'you', 'or', 'two', 'time', 'could', 'than', 'against', 'world', 'now', 'game', 'so', 'into', 'some', 'she', 'what', 'just', 'back', 'only', 'other', 'them', 'no', \"it's\", 'before', 'three', 'do', 'years', 'very', 'best', 'get', 'england', 'made', 'make', 'win', 'told', 'like', 'her', 'my', 'being', 'off']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"\\nNumber of words in the word index :\\n\\n{}\\n\\n\".format(len(word_index)))\n",
    "print(\"Words in order of commonality :\\n\\n{}\\n\".format(list(word_index)[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_padded_seq(idx):\n",
    "    print(\"\\nOriginal sentence : \\n\\n    {}\\n\\n\".format(sentences[idx]))\n",
    "    print(\"Padded tokenized version : \\n\\n    {}\\n\\n\".format(padded[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original sentence : \n",
      "\n",
      "    The budget deficit hit a record $412bn in the 12 months to 30 September 2004, after reaching $377bn in the previous fiscal year. The CBO also forecast a total shortfall of $855bn for the years from 2006 to 2015, an improvement on previous projections. However, analysts say the new figures fail to take into account the potential $2-$3.8 trillion costs of the president's plan to revamp state pensions and extend tax cuts. The figure could also be worsened by any further military costs. Republicans have blamed the size of the deficit on slow economic conditions after the 11 September attacks and ongoing military operations in Iraq and Afghanistan. One of President George W Bush's election pledges was to halve the budget deficit within five years. But Democrats have accused the president of excluding Iraq-related costs from previous budgets to meet the aim of reducing the deficit, a charge which the administration denies. On Tuesday, the US administration asked Congress for additional funds for military operations.\n",
      "\n",
      "\n",
      "\n",
      "Padded tokenized version : \n",
      "\n",
      "    [    2   548  1220   262     6   210 15123     7     2   360   223     3\n",
      "   447   732   161    45  2387 17633     7     2   645  2763    44     2\n",
      " 15119    46  1876     6   643  7449     4 23645     8     2    87    28\n",
      "   955     3  7876    32  3422    10   645 12879   197   509   172     2\n",
      "    49   381  3490     3   110    73  1188     2   768   125   145   429\n",
      "  2546   646     4     2  6390   622     3 10483   372  1782     5  2706\n",
      "   290  1124     2  1200    66    46    17  8431    24   121   398  2662\n",
      "   646 11620    19  2349     2  1728     4     2  1220    10  1631   324\n",
      "  1538    45     2   483   732   890     5  3423  2662  1727     7   917\n",
      "     5  5058    47     4   356  1465  2663  3940   177  4236    15     3\n",
      "  8432     2   548  1220   519   151    87    18  1539    19   881     2\n",
      "   356     4  6136   917  2917   646    28   645  5241     3   629     2\n",
      "  1829     4  2918     2  1220     6   895    39     2  2309  3582    10\n",
      "   544     2    52  2309   585  3347     8  2973  1748     8  2662  1727\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "see_padded_seq(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Padded Sequence Matrix : \n",
      "\n",
      "(Examples, Padded Size)\n",
      "    (16953, 450)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShape of Padded Sequence Matrix : \\n\\n{}\\n    {}\".format(\"(Examples, Padded Size)\",padded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words in the word index :\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "Words in order of commonality :\n",
      "\n",
      "['sport', 'business', 'politics', 'tech', 'entertainment', 'football', 'rugby', 'cricket', 'athletics', 'tennis']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_word_index = label_tokenizer.word_index\n",
    "print(\"\\nNumber of words in the word index :\\n\\n{}\\n\\n\".format(len(label_word_index)))\n",
    "print(\"Words in order of commonality :\\n\\n{}\\n\".format(list(label_word_index)[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sequences = label_tokenizer.texts_to_sequences(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_padded = pad_sequences(label_sequences, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_padded_label_seq(idx):\n",
    "    print(\"\\nOriginal label : \\n\\n    {}\\n\\n\".format(labels[idx]))\n",
    "    print(\"Padded tokenized version : \\n\\n    {}\\n\\n\".format(label_padded[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original label : \n",
      "\n",
      "    business\n",
      "\n",
      "\n",
      "Padded tokenized version : \n",
      "\n",
      "    [2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "see_padded_label_seq(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Padded Sequence Matrix : \n",
      "\n",
      "(Examples, Padded Size)\n",
      "    (2962, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShape of Padded Sequence Matrix : \\n\\n{}\\n    {}\".format(\"(Examples, Padded Size)\",label_padded.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
